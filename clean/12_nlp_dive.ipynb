{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukp0Mwlja7UW",
        "outputId": "722d0684-1fac-4679-ef0e-38ec1eff2197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "![ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YbnpMmxFa7UX"
      },
      "outputs": [],
      "source": [
        "from fastbook import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dunXy_Na7UX"
      },
      "source": [
        "# A Language Model from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eFfJx8va7UY"
      },
      "source": [
        "## The Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.text.all import *"
      ],
      "metadata": {
        "id": "ZVihCM5Lr1lk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URLs.HUMAN_NUMBERS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3pvxuyX2r58k",
        "outputId": "34303b3d-c171-4b00-f856-f8d03088c742"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://s3.amazonaws.com/fast-ai-sample/human_numbers.tgz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.HUMAN_NUMBERS)\n",
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "b31f6zyZr8t4",
        "outputId": "69a3e3fe-ded7-47ea-8077-86dbce7d4763"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='32768' class='' max='30252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      108.32% [32768/30252 00:00&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/root/.fastai/data/human_numbers')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Path.BASE_PATH = path\n",
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_o3ShxTsCI5",
        "outputId": "37400f31-ffe4-41a2-a6c5-ea7dacd91279"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('.')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path.ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaBf9k8nsFw8",
        "outputId": "aa91d644-13d1-420f-d09d-61f8f3d9dd3b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('valid.txt'),Path('train.txt')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = L()\n",
        "with open(path/'train.txt') as f:\n",
        "    lines += L(*f.readlines())\n",
        "with open(path/'valid.txt') as f:\n",
        "    lines += L(*f.readlines())\n",
        "lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW-td7UEsIvl",
        "outputId": "39978801-8a70-4bdf-edb7-b88a98bb3afc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQCto1lMsenn",
        "outputId": "d1bbb3d2-ca44-4359-e07c-b32e3bc0e272"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#10) ['nine thousand nine hundred ninety \\n','nine thousand nine hundred ninety one \\n','nine thousand nine hundred ninety two \\n','nine thousand nine hundred ninety three \\n','nine thousand nine hundred ninety four \\n','nine thousand nine hundred ninety five \\n','nine thousand nine hundred ninety six \\n','nine thousand nine hundred ninety seven \\n','nine thousand nine hundred ninety eight \\n','nine thousand nine hundred ninety nine \\n']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" . \".join([l.strip() for l in lines])\n",
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SyE9rbK7smfN",
        "outputId": "bb8a0278-63c3-40a2-a12b-cc09121514a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.split(\" \")\n",
        "tokens[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTR3-Qy7s24K",
        "outputId": "ab58f9aa-c56b-40c0-e0e2-b026a9b8d273"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W2twlRHs8fR",
        "outputId": "a9ba84bc-f1b5-4501-a1c9-109f8b296acb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63095"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1CZ6RvqtA5X",
        "outputId": "0852f420-1c86-4659-cbe8-1a782d256646"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hundred',\n",
              " 'ninety',\n",
              " 'eight',\n",
              " '.',\n",
              " 'nine',\n",
              " 'thousand',\n",
              " 'nine',\n",
              " 'hundred',\n",
              " 'ninety',\n",
              " 'nine']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = L(*tokens).unique()\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISvkp7rjtEAk",
        "outputId": "098866f1-01b3-40ed-b47c-877bec54c697"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "nums = L(word2idx[t] for t in tokens)\n",
        "nums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXuwOJ-utNRS",
        "outputId": "1d69ba64-4816-4149-f274-f8223960e597"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#63095) [0,1,2,1,3,1,4,1,5,1...]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The last index is `63094`."
      ],
      "metadata": {
        "id": "pP6hbd0ZiosR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nqho_dWa7UZ"
      },
      "source": [
        "## Our First Language Model from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens) - 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTyQeG0tvbac",
        "outputId": "aa49d0d3-3410-4db5-8571-1004b27a5121"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63091"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "range(0, len(tokens) - 4, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYmdUzeVvgqX",
        "outputId": "70fca7e4-1403-4d1d-fc94-ae5060f64c4c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 63091, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When `i` is `63091`, then `i + 3` is `63094`, which is the last index."
      ],
      "metadata": {
        "id": "TvAkL-_ji4dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L((tokens[i:i + 3], tokens[i + 3]) for i in range(0, len(tokens) - 4, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95xizVLbv099",
        "outputId": "9c93d981-13f4-40bb-d31d-bf5808ae08e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The slice `i:i + 3` isn't inclusive of `i + 3`. (It contains tokens `i`, `i + 1` and `i + 2`.) So we're using sequences of every 3 tokens as our independent variable, and the next token as our dependent variable."
      ],
      "metadata": {
        "id": "g7-M2IOBja7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For comparison:\n",
        "print(tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L3V1HUewAT1",
        "outputId": "82ed57f5-fce2-4cf4-814d-157843e768bf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.', 'six', '.', 'seven', '.', 'eight', '.', 'nine', '.', 'ten', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seqs = L((tensor(nums[i:i + 3]), nums[i + 3]) for i in range(0, len(nums) - 4, 3))\n",
        "seqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg0d8UDpwJL3",
        "outputId": "81d1877b-2f41-4607-901d-d8d6a75c66b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The above can be treated as a PyTorch `Dataset`, because it has a length, and supports indexing."
      ],
      "metadata": {
        "id": "jjDDCJG0l8OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 32\n",
        "cut = int(len(seqs) * 0.8)\n",
        "cut"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl0NkigEwnpL",
        "outputId": "128eadc8-18af-4cb4-f94f-b6cc43045dc8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16824"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=bs, shuffle=False)\n",
        "dls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfOSZdjGwxCi",
        "outputId": "4ae96b85-2708-41d6-ac82-83d29df0186c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<fastai.data.core.DataLoaders at 0x7c44d9be7370>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dls.train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mAu5l4Hw5sj",
        "outputId": "bbffdf92-577e-41a0-9999-afe887b7e5d9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<fastai.data.core.TfmdDL at 0x7c44d9be6ef0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(dls.train))\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uflDCBC2xbBD",
        "outputId": "ce2bf10f-b65f-46d7-fe08-a4c033fc3a97"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 3]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZNKm79-xgMx",
        "outputId": "25eb4fc5-1c21-4af3-d512-bcf82284e365"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2],\n",
              "        [1, 3, 1],\n",
              "        [4, 1, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The second row is a continuation of the sequence (after the end of the first row). This data prep isn't suitable for a stateful RNN."
      ],
      "metadata": {
        "id": "slioVxGdVfGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdzlhq_PxnFK",
        "outputId": "229a2f0f-154d-4f25-8d0f-22fb6a58af78"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 4, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** We don't have to one-hot encode the labels."
      ],
      "metadata": {
        "id": "w0kBwFtFlw16"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM9RzuQNa7Ua"
      },
      "source": [
        "### Our Language Model in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the forward pass works for `nn.Embedding`:"
      ],
      "metadata": {
        "id": "tP_wCZoPYURI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i_h = nn.Embedding(len(vocab), 64)\n",
        "i_h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-noH8Co0IiH",
        "outputId": "7606d0c8-1bf1-4b76-9f09-5f6cd28e28ef"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we pass in a batch of full sequences:"
      ],
      "metadata": {
        "id": "wKl8-_n8YcbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    h = i_h(x)\n",
        "h.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnYHZ5HL0Rvm",
        "outputId": "9f4560c3-c470-4b28-fb0b-91faa3729f10"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we pass in a batch of only the first token:"
      ],
      "metadata": {
        "id": "RsU579SVYj2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    h = i_h(x[:, 0])\n",
        "h.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMw0572h0mCR",
        "outputId": "7c4e33bb-98ff-4e84-ed3e-6849bb062797"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel1(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz) # Classification layer, a.k.a. 'head'.\n",
        "\n",
        "    def forward(self, x): # (bs, 3)\n",
        "        h = 0 + self.i_h(x[:, 0]) # (bs, n_hidden)\n",
        "        h = F.relu(self.h_h(h)) # (bs, n_hidden)\n",
        "        h = h + self.i_h(x[:, 1]) # (bs, n_hidden)\n",
        "        h = F.relu(self.h_h(h)) # (bs, n_hidden)\n",
        "        h = h + self.i_h(x[:, 2]) # (bs, n_hidden)\n",
        "        h = F.relu(self.h_h(h)) # (bs, n_hidden)\n",
        "        return self.h_o(h) # (bs, vocab_sz)"
      ],
      "metadata": {
        "id": "70qiL1yg0tk_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "\n",
        "1. This is a sequence-to-vector RNN. It produces an output only for the last token in the sequence, not for every token in the sequence.\n",
        "2. Each token is represented as a vector of length `n_hidden`. `n_hidden` can also be thought of as the number of channels. It should be treated as a hyperparameter.\n",
        "3. This network only works for sequences of length `3`. (We shall see how to change this shortly.)\n",
        "4. The slices `x[:, 0]`, `x[:, 1]` and `x[:, 2]` indicate that we're processing a batch at a time (like all neural networks), but only one token at a time.\n",
        "5. The `nn.Linear` layer `self.h_h` is re-used at each time step, because this linear layer is describing how the hidden state changes (gets updated) as we go from one token to the next. So this layer is expected to learn the weights to perform this computation (for all sorts of inputs).\n",
        "6. Just like in the case of `ResBlock`, the addition happens after the ReLU. (In this case, the input embedding vector of the next token is added to the ReLU activation of the current token. For this addition to work, the shapes of the two tensors must be the same. In other words, the `self.h_h` layer cannot change the number of channels, i.e., the length of the embedding vector.)\n",
        "7. PyTorch's `nn.RNNCell` class is (almost, but not quite) equivalent to applying `self.h_h` and the non-linearity. (It doesn't have the for loop built in! We have to write it ourselves.) See the math of the actual operation here: https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html\n",
        "8. And PyTorch's <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\" target=\"_blank\">nn.RNN</a> class is (almost) equivalent to stacking multiple `nn.RNNCell` layers. Only `nn.RNN` provides the `batch_first=True`, `dropout` and `bidirectional` options. If we use `nn.RNNCell`, we have to apply these manually ourselves. (The for loop is built in. We don't have to write it ourselves.)\n",
        "9. From the equations in the documentations of `nn.RNNCell` and `nn.RNN`, we see that the embedding of the current token is not directly added to the previous activation. Rather, it is passed through a linear layer first. The ReLU is applied after adding the outputs of the two linear layers (one for processing the input embeddings and one for processing the hidden state). This also means that the number of embedding dimensions CAN be changed by an RNN layer (because the two linear layers inside it have the same number of output units - the `hidden_size` argument, making addition of the outputs of the two linear layers possible).\n",
        "10. The input shape of `nn.RNN` is $(N, L, H_{in})$ when `batch_first=True`. In other words, it doesn't accept one token at a time. It only accepts a full sequence (of length $L$) - typically a batch of these. Its output shape is $(N, L, D∗H_{out})$, where $D=2$ if `bidirectional=True`, otherwise $1$. From the output shape, we see that it's a sequence-to-sequence RNN. But we can easily make it sequence-to-vector by ignoring the outputs of all but the last time step.\n",
        "11. In a simple RNN cell, the output (at any time step) and the hidden state (at any time step) are the same thing. In an LSTM / GRU cell, they are different."
      ],
      "metadata": {
        "id": "AXxE6Tf5onZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNNStateless(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, nonlinearity='tanh', return_sequences=False):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i_h = nn.Linear(input_size, hidden_size)\n",
        "        self.h_h = nn.Linear(hidden_size, hidden_size)\n",
        "        self.nonlinearity = nonlinearity\n",
        "        self.return_sequences = return_sequences\n",
        "\n",
        "    def forward(self, x): # (bs, seq_len, input_size)\n",
        "        bs, seq_len, input_size = x.shape\n",
        "        h = torch.zeros((bs, self.hidden_size)) # (bs, hidden_size)\n",
        "        all_hidden_states = []\n",
        "        for i in range(seq_len):\n",
        "            i_h = self.i_h(x[:, i]) # (bs, hidden_size)\n",
        "            h_h = self.h_h(h) # (bs, hidden_size)\n",
        "            if self.nonlinearity == 'tanh':\n",
        "                h = torch.tanh(i_h + h_h) # (bs, hidden_size)\n",
        "            else:\n",
        "                h = torch.relu(i_h + h_h) # (bs, hidden_size)\n",
        "            all_hidden_states.append(h)\n",
        "        if self.return_sequences:\n",
        "            return torch.stack(all_hidden_states, dim=1) # (bs, seq_len, hidden_size)\n",
        "        else:\n",
        "            return all_hidden_states[-1] # (bs, hidden_size)"
      ],
      "metadata": {
        "id": "SWS2P_wyAH8_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel1Alt(nn.Module):\n",
        "    def __init__(self, vocab_size, input_emb_size, hidden_size, nonlinearity='tanh', return_sequences=False):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, input_emb_size)\n",
        "        self.rnn = SimpleRNNStateless(input_emb_size, hidden_size, nonlinearity=nonlinearity, return_sequences=return_sequences)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x): # (bs, seq_len)\n",
        "        input_emb = self.emb(x) # (bs, seq_len, input_emb_size)\n",
        "        h = self.rnn(input_emb) # (bs, seq_len, hidden_size) if return_sequences == True else (bs, hidden_size)\n",
        "        return self.linear(h) # (bs, seq_len, vocab_size) if return_sequences == True else (bs, vocab_size)"
      ],
      "metadata": {
        "id": "eXF3U8NwB9rq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel1Alt2(nn.Module):\n",
        "    def __init__(self, vocab_size, input_emb_size, hidden_size, nonlinearity='tanh', return_sequences=False):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.emb = nn.Embedding(vocab_size, input_emb_size)\n",
        "        self.rnn = nn.RNNCell(input_emb_size, hidden_size, nonlinearity=nonlinearity)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.return_sequences = return_sequences\n",
        "\n",
        "    def forward(self, x): # (bs, seq_len)\n",
        "        input_emb = self.emb(x) # (bs, seq_len, input_emb_size)\n",
        "        bs, seq_len = x.shape\n",
        "        h = torch.zeros((bs, self.hidden_size)) # (bs, hidden_size)\n",
        "        all_hidden_states = []\n",
        "        for i in range(seq_len):\n",
        "            h = self.rnn(input_emb[:, i], h) # (bs, hidden_size)\n",
        "            all_hidden_states.append(h)\n",
        "        if self.return_sequences:\n",
        "            h = torch.stack(all_hidden_states, dim=1) # (bs, seq_len, hidden_size)\n",
        "        else:\n",
        "            h = all_hidden_states[-1] # (bs, hidden_size)\n",
        "        return self.linear(h) # (bs, seq_len, vocab_size) if return_sequences == True else (bs, vocab_size)"
      ],
      "metadata": {
        "id": "JdnR60ubGvTn"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel1Alt3(nn.Module):\n",
        "    def __init__(self, vocab_size, input_emb_size, hidden_size, nonlinearity='tanh', return_sequences=False):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.emb = nn.Embedding(vocab_size, input_emb_size)\n",
        "        self.rnn = nn.RNN(input_emb_size, hidden_size, num_layers=1, nonlinearity=nonlinearity, batch_first=True, dropout=0.0, bidirectional=False)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.return_sequences = return_sequences\n",
        "\n",
        "    def forward(self, x): # (bs, seq_len)\n",
        "        input_emb = self.emb(x) # (bs, seq_len, input_emb_size)\n",
        "        bs = x.shape[0]\n",
        "        h = torch.zeros((1, bs, self.hidden_size))\n",
        "        # h is the initial hidden state with shape (D * 1, bs, hidden_size); 1 is the number of layers.\n",
        "        # D = 2 if bidirectional == True else 1.\n",
        "        y, h = self.rnn(input_emb, h) # (bs, seq_len, hidden_size) is the shape of y.\n",
        "        # h is the last hidden state with shape (D * 1, bs, hidden_size); 1 is the number of layers.\n",
        "        # D = 2 if bidirectional == True else 1.\n",
        "        if not self.return_sequences:\n",
        "            y = y[:, -1] # (bs, hidden_size) if return_sequences == False\n",
        "        return self.linear(y) # (bs, seq_len, vocab_size) if return_sequences == True else (bs, vocab_size)"
      ],
      "metadata": {
        "id": "SyCdDOspSCDr"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LMModel1(len(vocab), 64)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrMu0l-04Dyx",
        "outputId": "3c1f8807-b0b0-4b53-b703-fb925726dba0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LMModel1(\n",
              "  (i_h): Embedding(30, 64)\n",
              "  (h_h): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (h_o): Linear(in_features=64, out_features=30, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test:\n",
        "with torch.no_grad():\n",
        "    output = model(x)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PDJ1-vI4JJM",
        "outputId": "9790a793-ed6e-4889-bf90-bd151e04e539"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_alt = LMModel1Alt(vocab_size=len(vocab), input_emb_size=64, hidden_size=128, nonlinearity='tanh', return_sequences=False)\n",
        "model_alt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JadTN5HdDqsB",
        "outputId": "54b37b0f-98d7-4068-fd48-be2d6371a1a7"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LMModel1Alt(\n",
              "  (emb): Embedding(30, 64)\n",
              "  (rnn): SimpleRNNStateless(\n",
              "    (i_h): Linear(in_features=64, out_features=128, bias=True)\n",
              "    (h_h): Linear(in_features=128, out_features=128, bias=True)\n",
              "  )\n",
              "  (linear): Linear(in_features=128, out_features=30, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test:\n",
        "with torch.no_grad():\n",
        "    output = model_alt(x)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS315sUSD6CT",
        "outputId": "aa197937-1d8a-475e-9944-910bd7f2fde3"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_alt2 = LMModel1Alt2(vocab_size=len(vocab), input_emb_size=64, hidden_size=128, nonlinearity='tanh', return_sequences=False)\n",
        "model_alt2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmYb_GCeL-nk",
        "outputId": "6c2455e0-54e0-4a69-9859-bda8c4831313"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LMModel1Alt2(\n",
              "  (emb): Embedding(30, 64)\n",
              "  (rnn): RNNCell(64, 128)\n",
              "  (linear): Linear(in_features=128, out_features=30, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test:\n",
        "with torch.no_grad():\n",
        "    output = model_alt2(x)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUwEHOAuMP6X",
        "outputId": "243ba2d8-8305-4a36-b22f-eb5a40e94a03"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_alt3 = LMModel1Alt3(vocab_size=len(vocab), input_emb_size=64, hidden_size=128, nonlinearity='tanh', return_sequences=False)\n",
        "model_alt3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swy_dZXYQ2Am",
        "outputId": "b3a12899-b902-4c06-f4ee-c2d885a4cafe"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LMModel1Alt3(\n",
              "  (emb): Embedding(30, 64)\n",
              "  (rnn): RNN(64, 128, batch_first=True)\n",
              "  (linear): Linear(in_features=128, out_features=30, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test:\n",
        "with torch.no_grad():\n",
        "    output = model_alt3(x)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMoS2kvZRZm3",
        "outputId": "0fd8fe4f-2704-44b4-83be-62613b97f5eb"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, model, loss_func=F.cross_entropy, metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "ke47z6hv4VRG",
        "outputId": "ac2a39e9-d36e-44d6-c7a5-5bb46747c012"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.398951</td>\n",
              "      <td>1.794102</td>\n",
              "      <td>0.487996</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.217450</td>\n",
              "      <td>1.848997</td>\n",
              "      <td>0.492512</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.295240</td>\n",
              "      <td>1.669092</td>\n",
              "      <td>0.502971</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.319708</td>\n",
              "      <td>1.543958</td>\n",
              "      <td>0.503209</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, model_alt, loss_func=F.cross_entropy, metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "-3eBqL5OEXWD",
        "outputId": "f5aaf235-b3ac-4b25-b121-d7ced6620808"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.356023</td>\n",
              "      <td>1.826325</td>\n",
              "      <td>0.501783</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.216416</td>\n",
              "      <td>1.716497</td>\n",
              "      <td>0.504160</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.237927</td>\n",
              "      <td>1.580869</td>\n",
              "      <td>0.492512</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.325362</td>\n",
              "      <td>1.456013</td>\n",
              "      <td>0.506774</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Elman RNN is performing slightly better than Jeremy's RNN."
      ],
      "metadata": {
        "id": "86R5ElOYz6va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, model_alt2, loss_func=F.cross_entropy, metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "oaft3LiVMYFP",
        "outputId": "a90890e7-8141-419c-b473-f8bde3983769"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.350925</td>\n",
              "      <td>1.948752</td>\n",
              "      <td>0.455669</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.217590</td>\n",
              "      <td>1.695616</td>\n",
              "      <td>0.505586</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.246906</td>\n",
              "      <td>1.538275</td>\n",
              "      <td>0.492750</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.325293</td>\n",
              "      <td>1.422045</td>\n",
              "      <td>0.506774</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, model_alt3, loss_func=F.cross_entropy, metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "Jp4kHjjxRijv",
        "outputId": "80e39194-bdba-4037-e3b1-0cce4bc94dcb"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.343593</td>\n",
              "      <td>1.851490</td>\n",
              "      <td>0.474210</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.207352</td>\n",
              "      <td>1.704578</td>\n",
              "      <td>0.505111</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.213180</td>\n",
              "      <td>1.603170</td>\n",
              "      <td>0.493701</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.323762</td>\n",
              "      <td>1.437765</td>\n",
              "      <td>0.506774</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Let's simulate one iteration of the loop below:"
      ],
      "metadata": {
        "id": "LhWdUkVVf89w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n, counts = 0, torch.zeros(len(vocab))\n",
        "counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaHejU0T47jq",
        "outputId": "a7b56f42-0255-421f-9074-038e1e490a50"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(dls.valid))\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKmUyBdk4_pD",
        "outputId": "ceaad12a-2f10-48e5-8fd5-2ab9592e2fa5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 3]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12cGBkZc5ELi",
        "outputId": "ea18e0c8-594c-4a8a-9879-54cecfbe0da9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n += y.shape[0]\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jgPDxNE5Gib",
        "outputId": "a4d39298-7c64-4447-8783-086a4e94a21c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(range_of(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4D29bBJ5MP0",
        "outputId": "3c21f86f-ed49-49b4-b5ac-25a5521f44b2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range_of(vocab):\n",
        "    counts[i] += (y == i).long().sum()\n",
        "counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPqmJOKq5PFb",
        "outputId": "dfbb72cc-5dc3-43d3-e6fc-aa4e3d879001"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 4., 1., 1., 1., 0., 1., 1., 6., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 3., 1., 8.])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "n9Y-AtEmggbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n, counts = 0, torch.zeros(len(vocab))\n",
        "for x, y in dls.valid:\n",
        "    n += y.shape[0]\n",
        "    for i in range_of(vocab):\n",
        "        counts[i] += (y == i).long().sum()\n",
        "counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9oXH74V5XYt",
        "outputId": "9f3aff81-af5c-493d-f760-745858538921"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([106., 637., 159., 107., 106., 159., 108., 106., 464., 442.,   6.,   7.,   6.,   6.,   7.,   6.,   6.,   7.,   6.,   6.,  64.,  63.,  63.,  64.,  63.,  63.,  66.,  66., 600., 638.])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.argmax(counts)\n",
        "idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czOsEi185rYJ",
        "outputId": "636172a4-4c03-46a8-8ecd-1ae678ae8304"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(29)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[idx.item()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RMtIp8H85uTd",
        "outputId": "ea517e64-8ef5-4e8c-b805-0c6b981e9c4e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thousand'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts[idx].item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct-9HBBu5wr9",
        "outputId": "50381054-270c-49b5-dfca-10cb4092fc70"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "638.0"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeJ1Qwzz50Py",
        "outputId": "236e11a2-0f88-45aa-ba0a-2a12db731c98"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4207"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts[idx].item() / n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyFZtR_m51V_",
        "outputId": "d5d01a9e-64e1-444b-e0f8-d063224a8eb1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15165200855716662"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVYvWCksa7Ua"
      },
      "source": [
        "### Our First Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel2(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = 0\n",
        "        for i in range(3):\n",
        "            h = h + self.i_h(x[:, i])\n",
        "            h = F.relu(self.h_h(h))\n",
        "        return self.h_o(h)"
      ],
      "metadata": {
        "id": "pOPnI7SSuOaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** If we had a situation where our batches had dynamic `seq_len`, then we could simply replace `range(3)` with `range(seq_len)` to make the above architecture work for ANY value of `seq_len`."
      ],
      "metadata": {
        "id": "l4BckIMQqBUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "38ozX7DjuxBh",
        "outputId": "8960f219-0236-49d0-8879-0145ff29f6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.809035</td>\n",
              "      <td>1.885577</td>\n",
              "      <td>0.464464</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.403376</td>\n",
              "      <td>1.710791</td>\n",
              "      <td>0.469218</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.405379</td>\n",
              "      <td>1.643671</td>\n",
              "      <td>0.492750</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.369966</td>\n",
              "      <td>1.620523</td>\n",
              "      <td>0.495365</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FbnyvCDa7Ub"
      },
      "source": [
        "## Improving the RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA22Ck4ia7Ub"
      },
      "source": [
        "### Maintaining the State of an RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2oyPpoEa7Ub"
      },
      "outputs": [],
      "source": [
        "class LMModel3(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "        self.h = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(3):\n",
        "            self.h = self.h + self.i_h(x[:,i])\n",
        "            self.h = F.relu(self.h_h(self.h))\n",
        "        out = self.h_o(self.h)\n",
        "        self.h = self.h.detach()\n",
        "        return out\n",
        "\n",
        "    def reset(self): self.h = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "embnUHrqa7Ub"
      },
      "outputs": [],
      "source": [
        "m = len(seqs)//bs\n",
        "m,bs,len(seqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEBzC9tUa7Ub"
      },
      "outputs": [],
      "source": [
        "def group_chunks(ds, bs):\n",
        "    m = len(ds) // bs\n",
        "    new_ds = L()\n",
        "    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n",
        "    return new_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dALWcAHxa7Ub"
      },
      "outputs": [],
      "source": [
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(\n",
        "    group_chunks(seqs[:cut], bs),\n",
        "    group_chunks(seqs[cut:], bs),\n",
        "    bs=bs, drop_last=True, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz1P5-f6a7Ub"
      },
      "outputs": [],
      "source": [
        "learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(10, 3e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoIwl1rca7Ub"
      },
      "source": [
        "### Creating More Signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uosDt8pRa7Uc"
      },
      "outputs": [],
      "source": [
        "sl = 16\n",
        "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
        "         for i in range(0,len(nums)-sl-1,sl))\n",
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n",
        "                             group_chunks(seqs[cut:], bs),\n",
        "                             bs=bs, drop_last=True, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvenA9sra7Uc"
      },
      "outputs": [],
      "source": [
        "[L(vocab[o] for o in s) for s in seqs[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thDlcpl3a7Uc"
      },
      "outputs": [],
      "source": [
        "class LMModel4(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "        self.h = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        for i in range(sl):\n",
        "            self.h = self.h + self.i_h(x[:,i])\n",
        "            self.h = F.relu(self.h_h(self.h))\n",
        "            outs.append(self.h_o(self.h))\n",
        "        self.h = self.h.detach()\n",
        "        return torch.stack(outs, dim=1)\n",
        "\n",
        "    def reset(self): self.h = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24Zvcak3a7Uc"
      },
      "outputs": [],
      "source": [
        "def loss_func(inp, targ):\n",
        "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWbRP2g-a7Uc"
      },
      "outputs": [],
      "source": [
        "learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOHOnVgua7Uc"
      },
      "source": [
        "## Multilayer RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPf9algaa7Uc"
      },
      "source": [
        "### The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75oCx-_fa7Uc"
      },
      "outputs": [],
      "source": [
        "class LMModel5(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h = torch.zeros(n_layers, bs, n_hidden)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res,h = self.rnn(self.i_h(x), self.h)\n",
        "        self.h = h.detach()\n",
        "        return self.h_o(res)\n",
        "\n",
        "    def reset(self): self.h.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in stateful RNNs in Keras, this network only works with a particular batch size."
      ],
      "metadata": {
        "id": "a_RD2pm_nbGl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J1n84Iua7Ui"
      },
      "outputs": [],
      "source": [
        "learn = Learner(dls, LMModel5(len(vocab), 64, 2),\n",
        "                loss_func=CrossEntropyLossFlat(),\n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SclCaj6a7Ui"
      },
      "source": [
        "### Exploding or Disappearing Activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APQw4JuVa7Ui"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pus4Lbb9a7Ui"
      },
      "source": [
        "### Building an LSTM from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LBiqM4-a7Ui"
      },
      "outputs": [],
      "source": [
        "class LSTMCell(Module):\n",
        "    def __init__(self, ni, nh):\n",
        "        self.forget_gate = nn.Linear(ni + nh, nh)\n",
        "        self.input_gate  = nn.Linear(ni + nh, nh)\n",
        "        self.cell_gate   = nn.Linear(ni + nh, nh)\n",
        "        self.output_gate = nn.Linear(ni + nh, nh)\n",
        "\n",
        "    def forward(self, input, state):\n",
        "        h,c = state\n",
        "        h = torch.cat([h, input], dim=1)\n",
        "        forget = torch.sigmoid(self.forget_gate(h))\n",
        "        c = c * forget\n",
        "        inp = torch.sigmoid(self.input_gate(h))\n",
        "        cell = torch.tanh(self.cell_gate(h))\n",
        "        c = c + inp * cell\n",
        "        out = torch.sigmoid(self.output_gate(h))\n",
        "        h = out * torch.tanh(c)\n",
        "        return h, (h,c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko22FWdpa7Ui"
      },
      "outputs": [],
      "source": [
        "class LSTMCell(Module):\n",
        "    def __init__(self, ni, nh):\n",
        "        self.ih = nn.Linear(ni,4*nh)\n",
        "        self.hh = nn.Linear(nh,4*nh)\n",
        "\n",
        "    def forward(self, input, state):\n",
        "        h,c = state\n",
        "        # One big multiplication for all the gates is better than 4 smaller ones\n",
        "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n",
        "        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n",
        "        cellgate = gates[3].tanh()\n",
        "\n",
        "        c = (forgetgate*c) + (ingate*cellgate)\n",
        "        h = outgate * c.tanh()\n",
        "        return h, (h,c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgRIB_dKa7Uj"
      },
      "outputs": [],
      "source": [
        "t = torch.arange(0,10); t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp-U8eVsa7Uj"
      },
      "outputs": [],
      "source": [
        "t.chunk(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbtYzmi5a7Uj"
      },
      "source": [
        "### Training a Language Model Using LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJNcmNwla7Uj"
      },
      "outputs": [],
      "source": [
        "class LMModel6(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "\n",
        "    def forward(self, x):\n",
        "        res,h = self.rnn(self.i_h(x), self.h)\n",
        "        self.h = [h_.detach() for h_ in h]\n",
        "        return self.h_o(res)\n",
        "\n",
        "    def reset(self):\n",
        "        for h in self.h: h.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAGwLszBa7Uj"
      },
      "outputs": [],
      "source": [
        "learn = Learner(dls, LMModel6(len(vocab), 64, 2),\n",
        "                loss_func=CrossEntropyLossFlat(),\n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVFucYnda7Uj"
      },
      "source": [
        "## Regularizing an LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDAWOpCXa7Uj"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNLQv3_Ba7Uj"
      },
      "outputs": [],
      "source": [
        "class Dropout(Module):\n",
        "    def __init__(self, p): self.p = p\n",
        "    def forward(self, x):\n",
        "        if not self.training: return x\n",
        "        mask = x.new(*x.shape).bernoulli_(1-p)\n",
        "        return x * mask.div_(1-p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO-LACXaa7Uj"
      },
      "source": [
        "### Activation Regularization and Temporal Activation Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBZJW5PKa7Uj"
      },
      "source": [
        "### Training a Weight-Tied Regularized LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QVzHuIDa7Uj"
      },
      "outputs": [],
      "source": [
        "class LMModel7(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.drop = nn.Dropout(p)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h_o.weight = self.i_h.weight\n",
        "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "\n",
        "    def forward(self, x):\n",
        "        raw,h = self.rnn(self.i_h(x), self.h)\n",
        "        out = self.drop(raw)\n",
        "        self.h = [h_.detach() for h_ in h]\n",
        "        return self.h_o(out),raw,out\n",
        "\n",
        "    def reset(self):\n",
        "        for h in self.h: h.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W32SKg3ma7Uk"
      },
      "outputs": [],
      "source": [
        "learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n",
        "                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n",
        "                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efyPz0xca7Uk"
      },
      "outputs": [],
      "source": [
        "learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n",
        "                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIBatbc2a7Uk"
      },
      "outputs": [],
      "source": [
        "learn.fit_one_cycle(15, 1e-2, wd=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOpg2vb1a7Uk"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTdgCa30a7Uk"
      },
      "source": [
        "## Questionnaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkJaZci3a7Uk"
      },
      "source": [
        "1. If the dataset for your project is so big and complicated that working with it takes a significant amount of time, what should you do?\n",
        "1. Why do we concatenate the documents in our dataset before creating a language model?\n",
        "1. To use a standard fully connected network to predict the fourth word given the previous three words, what two tweaks do we need to make to our model?\n",
        "1. How can we share a weight matrix across multiple layers in PyTorch?\n",
        "1. Write a module that predicts the third word given the previous two words of a sentence, without peeking.\n",
        "1. What is a recurrent neural network?\n",
        "1. What is \"hidden state\"?\n",
        "1. What is the equivalent of hidden state in ` LMModel1`?\n",
        "1. To maintain the state in an RNN, why is it important to pass the text to the model in order?\n",
        "1. What is an \"unrolled\" representation of an RNN?\n",
        "1. Why can maintaining the hidden state in an RNN lead to memory and performance problems? How do we fix this problem?\n",
        "1. What is \"BPTT\"?\n",
        "1. Write code to print out the first few batches of the validation set, including converting the token IDs back into English strings, as we showed for batches of IMDb data in <<chapter_nlp>>.\n",
        "1. What does the `ModelResetter` callback do? Why do we need it?\n",
        "1. What are the downsides of predicting just one output word for each three input words?\n",
        "1. Why do we need a custom loss function for `LMModel4`?\n",
        "1. Why is the training of `LMModel4` unstable?\n",
        "1. In the unrolled representation, we can see that a recurrent neural network actually has many layers. So why do we need to stack RNNs to get better results?\n",
        "1. Draw a representation of a stacked (multilayer) RNN.\n",
        "1. Why should we get better results in an RNN if we call `detach` less often? Why might this not happen in practice with a simple RNN?\n",
        "1. Why can a deep network result in very large or very small activations? Why does this matter?\n",
        "1. In a computer's floating-point representation of numbers, which numbers are the most precise?\n",
        "1. Why do vanishing gradients prevent training?\n",
        "1. Why does it help to have two hidden states in the LSTM architecture? What is the purpose of each one?\n",
        "1. What are these two states called in an LSTM?\n",
        "1. What is tanh, and how is it related to sigmoid?\n",
        "1. What is the purpose of this code in `LSTMCell`: `h = torch.cat([h, input], dim=1)`\n",
        "1. What does `chunk` do in PyTorch?\n",
        "1. Study the refactored version of `LSTMCell` carefully to ensure you understand how and why it does the same thing as the non-refactored version.\n",
        "1. Why can we use a higher learning rate for `LMModel6`?\n",
        "1. What are the three regularization techniques used in an AWD-LSTM model?\n",
        "1. What is \"dropout\"?\n",
        "1. Why do we scale the acitvations with dropout? Is this applied during training, inference, or both?\n",
        "1. What is the purpose of this line from `Dropout`: `if not self.training: return x`\n",
        "1. Experiment with `bernoulli_` to understand how it works.\n",
        "1. How do you set your model in training mode in PyTorch? In evaluation mode?\n",
        "1. Write the equation for activation regularization (in math or code, as you prefer). How is it different from weight decay?\n",
        "1. Write the equation for temporal activation regularization (in math or code, as you prefer). Why wouldn't we use this for computer vision problems?\n",
        "1. What is \"weight tying\" in a language model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLt7p7BFa7Uk"
      },
      "source": [
        "### Further Research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KlmNpmpa7Uk"
      },
      "source": [
        "1. In ` LMModel2`, why can `forward` start with `h=0`? Why don't we need to say `h=torch.zeros(...)`?\n",
        "1. Write the code for an LSTM from scratch (you may refer to <<lstm>>).\n",
        "1. Search the internet for the GRU architecture and implement it from scratch, and try training a model. See if you can get results similar to those we saw in this chapter. Compare your results to the results of PyTorch's built in `GRU` module.\n",
        "1. Take a look at the source code for AWD-LSTM in fastai, and try to map each of the lines of code to the concepts shown in this chapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H7r0cK-a7Uk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}